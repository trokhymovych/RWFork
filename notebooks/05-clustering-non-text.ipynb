{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6240e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65abe5736b3c4ec28d015844b86964eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 31.5 s, total: 1min 42s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import altair as alt\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "from fuzzywuzzy import fuzz  # type: ignore\n",
    "from itertools import product\n",
    "\n",
    "extracted_data = joblib.load(\"../data/ru_wiki_extracted_pages.data\")\n",
    "error_data = joblib.load(\"../data/ru_wiki_error_pages.data\")\n",
    "final_data = pd.DataFrame(joblib.load(\"../data/ru_wiki_final_dataset_v2.data\"))\n",
    "with open('../data/ru_reveal_wiki_location.json') as user_file:\n",
    "    ru_reveal_wiki_location = json.load(user_file)\n",
    "locations_dict = dict()\n",
    "for location in tqdm(ru_reveal_wiki_location):\n",
    "    location_key = list(location.keys())[0]\n",
    "    location_values = np.sort(list(location.values())[0])\n",
    "    locations_dict[location_key] = \"_\".join(location_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aad6b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ea893f73434a6db7e58f02ab59ed96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1924975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../data/ru_reveal_wiki_topic.json') as user_file:\n",
    "    ru_reveal_wiki_topic = json.load(user_file)\n",
    "topics_dict = {list(d.keys())[0]: [v['topic'] for v in list(d.values())[0]] for d in tqdm(ru_reveal_wiki_topic)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e7fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {a[\"page_name\"]: a[\"revision_details\"][\"user\"] for a in extracted_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399bcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return text\n",
    "    # Remove \\n characters\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # Remove redundant spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_template(text):\n",
    "    return text\n",
    "\n",
    "def get_template_parameters(templates):\n",
    "    all_parameters = {}\n",
    "    for t in templates:\n",
    "        # Split the template string on the '|' character\n",
    "        parts = t.split('|')\n",
    "\n",
    "        if parts:\n",
    "            # The first part is assumed to be the template name\n",
    "            template_name = parts[0]\n",
    "\n",
    "            # Initialize a dictionary for the current template\n",
    "            template_parameters = {}\n",
    "\n",
    "            # Process the remaining parts as parameter key-value pairs\n",
    "            for param_pair in parts[1:]:\n",
    "                param_split = param_pair.split('=')\n",
    "                param_name, param_value = param_split[0], \"\".join(param_split[1:])\n",
    "                # Create a combined key using the template name and parameter name\n",
    "                combined_key = f\"{template_name}+{param_name}\"\n",
    "                template_parameters[clean_text(combined_key)] = clean_text(param_value)\n",
    "\n",
    "            # Add the template name and its parameters to the dictionary\n",
    "            all_parameters.update(template_parameters)\n",
    "\n",
    "    return all_parameters\n",
    "\n",
    "\n",
    "def compare_templates(t1, t2): \n",
    "    params1 = get_template_parameters([t1])\n",
    "    params2 = get_template_parameters([t2])\n",
    "    changes = []\n",
    "    for i in params1.keys():\n",
    "        if params2.get(i) != params1.get(i):\n",
    "            param_to_add = (i, params1.get(i), params2.get(i))\n",
    "            if param_to_add not in changes:\n",
    "                changes.append(param_to_add)\n",
    "                \n",
    "    for i in params2.keys():\n",
    "        if params2.get(i) != params1.get(i):\n",
    "            param_to_add = (i, params1.get(i), params2.get(i))\n",
    "            if param_to_add not in changes:\n",
    "                changes.append(param_to_add)\n",
    "    \n",
    "    return changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257391d4",
   "metadata": {},
   "source": [
    "# Clustering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_df = final_data[final_data.status.isin([3])].reset_index(drop=True)\n",
    "action_features = pd.DataFrame(changed_df[\"actions\"].to_list())\n",
    "status_features = changed_df[[\"status\"]].reset_index(drop=True)\n",
    "n_added = changed_df.lines_added.apply(len).reset_index(drop=True)\n",
    "n_removed = changed_df.lines_deleted.apply(len).reset_index(drop=True)\n",
    "n_changed = changed_df.lines_changed.apply(len).reset_index(drop=True)\n",
    "\n",
    "n_added = changed_df.lines_added.apply(lambda x: len(x)>0).reset_index(drop=True)\n",
    "n_removed = changed_df.lines_deleted.apply(lambda x: len(x)>0).reset_index(drop=True)\n",
    "n_changed = changed_df.lines_changed.apply(lambda x: len(x)>0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Templates\n",
    "tem_added, tem_deleted, param_change = [], [], []\n",
    "tem_added_counter, tem_deleted_counter, param_change_counter = Counter(), Counter(), Counter()\n",
    "for wiki_features, ruwiki_features in \\\n",
    "    tqdm(zip(changed_df[\"wiki_features\"].to_list(), changed_df[\"ruwiki_features\"].to_list())):\n",
    "    tem_added.append(set([clean_template(i) for i in ruwiki_features[\"templates\"]]) - set([clean_template(i) for i in wiki_features[\"templates\"]]) - set([None]))\n",
    "    tem_deleted.append(set([clean_template(i) for i in wiki_features[\"templates\"]]) - set([clean_template(i) for i in ruwiki_features[\"templates\"]]) - set([None]))\n",
    "    tem_added_counter.update(tem_added[-1])\n",
    "    tem_deleted_counter.update(tem_deleted[-1])\n",
    "    \n",
    "    params_changed = []\n",
    "    \n",
    "    tem_added_tmp = set(ruwiki_features[\"templates\"]) - set(wiki_features[\"templates\"]) - set([None])\n",
    "    tem_deleted_tmp = set(wiki_features[\"templates\"]) - set(ruwiki_features[\"templates\"]) - set([None])\n",
    "    for t1, t2 in product(tem_added_tmp, tem_deleted_tmp):\n",
    "        similarity = fuzz.ratio(t1, t2)\n",
    "        if similarity > 60 and similarity < 100:\n",
    "            try:\n",
    "                params_changed += compare_templates(t1, t2)\n",
    "            except:\n",
    "                print(t1)\n",
    "                pass\n",
    "    param_change.append(params_changed)\n",
    "    param_change_counter.update(params_changed)\n",
    "    \n",
    "added_most_common = [i[0] for i in tem_added_counter.most_common(15)]\n",
    "deleted_most_common = [i[0] for i in tem_deleted_counter.most_common(15)]\n",
    "change_key = Counter()\n",
    "for li in param_change:\n",
    "    for a, b, c in li:\n",
    "        change_key.update([a])\n",
    "        change_key.update([b])\n",
    "        change_key.update([c])\n",
    "\n",
    "keys_most_common = [i[0] for i in change_key.most_common(15)]\n",
    "tag_a_features = pd.DataFrame([{c: c in cat_list for c in added_most_common} for cat_list in tem_added]).reset_index(drop=True)\n",
    "tag_d_features = pd.DataFrame([{c: c in cat_list for c in deleted_most_common} for cat_list in tem_deleted]).reset_index(drop=True)\n",
    "tag_c_features = pd.DataFrame([{c: c in ([a[0] for a in cat_list]) or (c in [a[1] for a in cat_list]) or (c in [a[2] for a in cat_list])\n",
    "                                for c in keys_most_common} for cat_list in param_change]).reset_index(drop=True)\n",
    "changed_df[\"tag_cat\"] = tem_added    \n",
    "\n",
    "# Category\n",
    "categories_added, categories_removed, common_cat = [], [], []\n",
    "categories_counter, common_categories_counter = Counter(), Counter()\n",
    "for wiki_features, ruwiki_features in \\\n",
    "    zip(changed_df[\"wiki_features\"].to_list(), changed_df[\"ruwiki_features\"].to_list()):\n",
    "    categories_added.append(set(ruwiki_features[\"categories\"]) - set(wiki_features[\"categories\"]))\n",
    "    categories_removed.append(set(wiki_features[\"categories\"]) - set(ruwiki_features[\"categories\"]))\n",
    "    common_cat.append(set(wiki_features[\"categories\"]) & set(ruwiki_features[\"categories\"]))\n",
    "    categories_counter.update(categories_added[-1])\n",
    "    categories_counter.update(categories_removed[-1])\n",
    "    common_categories_counter.update(common_cat[-1])\n",
    "\n",
    "changed_df[\"rm_cat\"] = categories_removed\n",
    "categories_most_common = [i[0] for i in categories_counter.most_common(5)]\n",
    "categories_features = pd.DataFrame([{c: c in cat_list for c in categories_most_common} for cat_list in categories_removed]).reset_index(drop=True)\n",
    "    \n",
    "# topic\n",
    "topics = changed_df.page_name.apply(lambda d: list(np.sort(topics_dict.get(d, []))))\n",
    "topic_counter = Counter()\n",
    "for i in topics:\n",
    "    topic_counter.update(i)\n",
    "topics_features = pd.DataFrame([{c: c in cat_list for c in topic_counter.keys()} for cat_list in topics]).reset_index(drop=True)\n",
    "\n",
    "# location\n",
    "locations = changed_df.page_name.apply(lambda d: locations_dict.get(d, \"unknown\")).reset_index(drop=True)\n",
    "changed_df[\"locations\"] = locations\n",
    "repl = locations.value_counts()[20:].index\n",
    "locations_features = pd.get_dummies(locations.replace(repl, 'uncommon')).reset_index(drop=True)\n",
    "\n",
    "# user\n",
    "user_features = changed_df.page_name.apply(lambda d: user_dict.get(d, \"unknown\")).reset_index(drop=True)\n",
    "changed_df[\"user\"] = user_features\n",
    "repl = user_features.value_counts()[20:].index\n",
    "user_features = pd.get_dummies(user_features.replace(repl, 'uncommon')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cef7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('ru')\n",
    "\n",
    "def clean(text):\n",
    "    # Define a regular expression pattern to match brackets and punctuation\n",
    "    pattern = r'[()\\[\\]{}<>.,;!?:\"\\'-]'\n",
    "    # Use re.sub() to replace matches with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "most_common_words = Counter()\n",
    "for i in changed_df.page_name.apply(lambda x: [a for a in clean(x).split() if a not in stop_words]):\n",
    "    most_common_words.update(i)\n",
    "    \n",
    "common_words = [a[0] for a in most_common_words.most_common(50)]\n",
    "\n",
    "words = changed_df.page_name.apply(lambda x: {w: w in x for w in common_words})\n",
    "words_features = pd.DataFrame(words.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adaae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([\n",
    "    locations_features.reset_index(drop=True), \n",
    "    action_features.reset_index(drop=True),\n",
    "    tag_a_features.reset_index(drop=True),\n",
    "    tag_d_features.reset_index(drop=True),\n",
    "    tag_c_features.reset_index(drop=True),\n",
    "    categories_features.reset_index(drop=True),\n",
    "], axis=1)\n",
    "\n",
    "features.columns = [x.replace(\" \", \"_\") if x else \"None\" for x in features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ab8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.21394143 0.15233972 0.1226794  0.08445715 0.05485501 0.05093407\n",
      " 0.04617379 0.04013948 0.02790248 0.02640415 0.02287483 0.01569849\n",
      " 0.00916852 0.00756126 0.00694566 0.00690997 0.00654279 0.0056737\n",
      " 0.00513964 0.00482592 0.00435439 0.00401216 0.00386696 0.00352471\n",
      " 0.00348072 0.00302859 0.00297924 0.00282755 0.00276381 0.00248979]\n",
      "Explained Variance Ratio: 0.9444953945248958\n",
      "Principal Components (Eigenvectors):\n",
      "[[-3.28538535e-03 -1.27113476e-02 -2.63343314e-03 ...  1.95648839e-03\n",
      "  -0.00000000e+00  1.65329319e-03]\n",
      " [-2.60715989e-03  3.47265307e-01 -1.91222951e-03 ...  6.57898912e-03\n",
      "  -0.00000000e+00  7.18163448e-03]\n",
      " [-4.17538625e-03 -1.68015328e-01 -3.04979932e-03 ...  1.75122647e-03\n",
      "  -0.00000000e+00  2.50503254e-04]\n",
      " ...\n",
      " [-8.99242682e-03  1.12868470e-02  2.12324292e-03 ... -1.46987810e-01\n",
      "   0.00000000e+00 -9.11265334e-03]\n",
      " [ 8.34438355e-03  5.89916867e-03  2.62218498e-02 ...  8.37258908e-01\n",
      "  -0.00000000e+00  1.11993891e-02]\n",
      " [ 9.29995392e-03 -1.58345337e-02  2.01676403e-02 ... -1.70635025e-02\n",
      "   0.00000000e+00 -3.53218167e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pca = PCA(n_components=30)\n",
    "# Fit the PCA model to your data and transform the data to the new feature space\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(features)\n",
    "transformed_data = pca.fit_transform(normalized_data)\n",
    "# The transformed_data variable now contains your data in the reduced feature space\n",
    "# You can also access the explained variance ratio for each component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Explained Variance Ratio:\", np.sum(explained_variance_ratio))\n",
    "\n",
    "# Optionally, you can access the principal components themselves\n",
    "principal_components = pca.components_\n",
    "# Print the principal components (eigenvectors)\n",
    "print(\"Principal Components (Eigenvectors):\")\n",
    "print(principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd9521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, HDBSCAN, AgglomerativeClustering\n",
    "model = DBSCAN(eps=0.7, min_samples=10)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(transformed_data)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546fe3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562ed28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "changed_df[\"cluster\"] = yhat\n",
    "\n",
    "for i in unique(yhat):\n",
    "    if len(changed_df[changed_df.cluster == i]) > 100:\n",
    "        print(i)\n",
    "        print(len(changed_df[changed_df.cluster == i]))\n",
    "        print(changed_df[changed_df.cluster == i].status.value_counts())\n",
    "        if len(changed_df[changed_df.cluster == i]) > 10:  \n",
    "            display(changed_df[changed_df.cluster == i].sample(10))\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c349d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_df[[\"page_name\", \"locations\", \"user\", \"cluster\"]].to_csv(\"clusters_notext_no_users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ff67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

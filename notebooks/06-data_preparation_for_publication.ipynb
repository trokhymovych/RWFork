{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3959f9b",
   "metadata": {},
   "source": [
    "# Data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0eaeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca34ee5755e241eea5ea0edcae1b7fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ac8b8343e84cb885ad59116484ec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1924975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 58.8 s, total: 2min 20s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import glob\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import altair as alt\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "extracted_data = joblib.load(\"../data/ru_wiki_extracted_pages.data\")\n",
    "error_data = joblib.load(\"../data/ru_wiki_error_pages.data\")\n",
    "final_data = pd.DataFrame(joblib.load(\"../data/ru_wiki_final_dataset_v2.data\"))\n",
    "with open('../data/ru_reveal_wiki_location.json') as user_file:\n",
    "    ru_reveal_wiki_location = json.load(user_file)\n",
    "locations_dict = dict()\n",
    "for location in tqdm(ru_reveal_wiki_location):\n",
    "    location_key = list(location.keys())[0]\n",
    "    location_values = np.sort(list(location.values())[0])\n",
    "    locations_dict[location_key] = \"_\".join(location_values)\n",
    "\n",
    "with open('../data/ru_reveal_wiki_topic.json') as user_file:\n",
    "    ru_reveal_wiki_topic = json.load(user_file)\n",
    "topics_dict = {list(d.keys())[0]: [v['topic'] for v in list(d.values())[0]] for d in tqdm(ru_reveal_wiki_topic)}\n",
    "\n",
    "# Processing data: \n",
    "changed_df = final_data[final_data.status.isin([3, 4])]\n",
    "not_found_df = final_data[final_data.status.isin([1])]\n",
    "action_features = pd.DataFrame(changed_df[\"actions\"].to_list())\n",
    "status_features = changed_df[[\"status\"]].reset_index(drop=True)\n",
    "n_added = changed_df.lines_added.apply(len).reset_index(drop=True)\n",
    "n_removed = changed_df.lines_deleted.apply(len).reset_index(drop=True)\n",
    "n_changed = changed_df.lines_changed.apply(len).reset_index(drop=True)\n",
    "\n",
    "# Category\n",
    "categories_added, categories_removed = [], []\n",
    "categories_counter = Counter()\n",
    "for wiki_features, ruwiki_features in \\\n",
    "    zip(changed_df[\"wiki_features\"].to_list(), changed_df[\"ruwiki_features\"].to_list()):\n",
    "    categories_added.append(set(ruwiki_features[\"categories\"]) - set(wiki_features[\"categories\"]))\n",
    "    categories_removed.append(set(wiki_features[\"categories\"]) - set(ruwiki_features[\"categories\"])) \n",
    "    categories_counter.update(categories_added[-1])\n",
    "    categories_counter.update(categories_removed[-1])\n",
    "    \n",
    "# topic\n",
    "topics = changed_df.page_name.apply(lambda d: list(np.sort(topics_dict.get(d, [])))).values\n",
    "# location\n",
    "locations = changed_df.page_name.apply(lambda d: locations_dict.get(d, \"unknown\")).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34f65be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from fuzzywuzzy import fuzz  # type: ignore\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return text\n",
    "    # Remove \\n characters\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # Remove redundant spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_template(text):\n",
    "    return text\n",
    "\n",
    "def get_template_parameters(templates):\n",
    "    all_parameters = {}\n",
    "    for t in templates:\n",
    "        # Split the template string on the '|' character\n",
    "        parts = t.split('|')\n",
    "\n",
    "        if parts:\n",
    "            # The first part is assumed to be the template name\n",
    "            template_name = parts[0]\n",
    "\n",
    "            # Initialize a dictionary for the current template\n",
    "            template_parameters = {}\n",
    "\n",
    "            # Process the remaining parts as parameter key-value pairs\n",
    "            for param_pair in parts[1:]:\n",
    "                param_split = param_pair.split('=')\n",
    "                param_name, param_value = param_split[0], \"\".join(param_split[1:])\n",
    "                # Create a combined key using the template name and parameter name\n",
    "                combined_key = f\"{template_name}+{param_name}\"\n",
    "                template_parameters[clean_text(combined_key)] = clean_text(param_value)\n",
    "\n",
    "            # Add the template name and its parameters to the dictionary\n",
    "            all_parameters.update(template_parameters)\n",
    "\n",
    "    return all_parameters\n",
    "\n",
    "\n",
    "def compare_templates(t1, t2): \n",
    "    params1 = get_template_parameters([t1])\n",
    "    params2 = get_template_parameters([t2])\n",
    "    changes = []\n",
    "    for i in params1.keys():\n",
    "        if params2.get(i) != params1.get(i):\n",
    "            param_to_add = (i, params1.get(i), params2.get(i))\n",
    "            if param_to_add not in changes:\n",
    "                changes.append(param_to_add)\n",
    "                \n",
    "    for i in params2.keys():\n",
    "        if params2.get(i) != params1.get(i):\n",
    "            param_to_add = (i, params1.get(i), params2.get(i))\n",
    "            if param_to_add not in changes:\n",
    "                changes.append(param_to_add)\n",
    "    \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0e16fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd382c7c3ff4e1dbc95d80e34156a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_count = 0\n",
    "tem_added, tem_deleted, param_change = [], [], []\n",
    "tem_added_counter, tem_deleted_counter, param_change_counter = Counter(), Counter(), Counter()\n",
    "for wiki_features, ruwiki_features in \\\n",
    "    tqdm(zip(changed_df[\"wiki_features\"].to_list(), changed_df[\"ruwiki_features\"].to_list())):\n",
    "    tem_added.append(set([clean_template(i) for i in ruwiki_features[\"templates\"]]) - set([clean_template(i) for i in wiki_features[\"templates\"]]) - set([None]))\n",
    "    tem_deleted.append(set([clean_template(i) for i in wiki_features[\"templates\"]]) - set([clean_template(i) for i in ruwiki_features[\"templates\"]]) - set([None]))\n",
    "    tem_added_counter.update(tem_added[-1])\n",
    "    tem_deleted_counter.update(tem_deleted[-1])\n",
    "    \n",
    "    params_changed = []\n",
    "    \n",
    "    tem_added_tmp = set(ruwiki_features[\"templates\"]) - set(wiki_features[\"templates\"]) - set([None])\n",
    "    tem_deleted_tmp = set(wiki_features[\"templates\"]) - set(ruwiki_features[\"templates\"]) - set([None])\n",
    "    for t1, t2 in product(tem_added_tmp, tem_deleted_tmp):\n",
    "        similarity = fuzz.ratio(t1, t2)\n",
    "        if similarity > 60 and similarity < 100:\n",
    "            try:\n",
    "                # remove similar templates from the list (as canges will be added to the list)\n",
    "                tem_added[-1] = tem_added[-1] - set([t1])\n",
    "                tem_deleted[-1] = tem_deleted[-1] - set([t2])\n",
    "                params_changed += compare_templates(t1, t2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                error_count += 1\n",
    "                pass\n",
    "    param_change.append(params_changed)\n",
    "    param_change_counter.update(params_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d71b38c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0709cf2b264919a00e251e39c37894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all file names:\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_json(text):\n",
    "    text = text.replace('\"desc\": \" ', '\"desc\": \"')\n",
    "    text = text.replace(\"\\'Донецкой области\\' на \\'Донецкой Народной Республике\\'\", \"Донецкой области на Донецкой Народной Республике\")\n",
    "    text = text.replace('\"Радивоновка\"', 'Радивоновка').replace('\"Акимовском районе\"', 'Акимовском районе').replace('\"Мелитополе\"', 'Мелитополе')\n",
    "    text = text.replace('```json\\n', '').replace('\\n```', '').replace(',\\n}', '}').replace('\" \"', ' empty double quotes ')\n",
    "    text = text.replace('\\\"буржуек\\\"', '\\\\\\\"буржуек\\\\\\\"').replace(' \" ', ' double quotes  ').replace(\"”\", '\"')\n",
    "    text = text.replace('\"будут изображены\" to \"изображены\"', 'будут изображены to изображены')\n",
    "    text = text.replace('55°47\\'32\"N 37°36\\'15\"E.', \"\").replace(\"\\\\\\'Маньяк\\\\\\'\", \"Маньяк\").replace(\")\\n}\", \"\\n}\")\n",
    "    text = text.replace('\"нерешительное и дилетантское\"', 'нерешительное и дилетантское')\n",
    "    \n",
    "    return json.loads(text)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     return None\n",
    "\n",
    "files = glob.glob(\"../data/embed/output/summary/batch_*.jsonl\")\n",
    "responses = []\n",
    "for output_file in files:\n",
    "    # Read responses from jsonl file\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            responses.append(json.loads(line))\n",
    "\n",
    "summaries = []\n",
    "ids = []\n",
    "for response in tqdm(responses):\n",
    "    summaries.append(parse_json(response[\"response\"][\"body\"][\"choices\"][0]['message'][\"content\"]))\n",
    "    ids.append(response[\"custom_id\"])\n",
    "\n",
    "summary_dict = dict(zip(ids, summaries))\n",
    "\n",
    "# Extracting requests and their ids:\n",
    "jsonl_files_input = glob.glob(\"../data/embed/input/batch_requests_summary*.jsonl\")\n",
    "\n",
    "ids_input = []\n",
    "texts_input = []\n",
    "for file in jsonl_files_input:\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            request = json.loads(line)\n",
    "            ids_input.append(request[\"custom_id\"])\n",
    "            texts_input.append(request[\"body\"][\"messages\"][1][\"content\"][1:-1])\n",
    "\n",
    "# Create a dataframe with the embeddings and the ids\n",
    "text_to_summary = {text: summary_dict.get(id, None)['desc'] for text, id in zip(texts_input, ids_input) if summary_dict.get(id, None) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b7f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_page_title(text):\n",
    "    try:\n",
    "        return re.findall(r'page title: (.*)\\nchange type', text)[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        return None\n",
    "\n",
    "# Modify the dictionary to include the page title in keys: \n",
    "text_to_summary = {extract_page_title(k): v for k, v in text_to_summary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c89bd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/embed/output/embedding_summary/batch_*.jsonl\")\n",
    "responses = []\n",
    "for output_file in files:\n",
    "    # Read responses from jsonl file\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            responses.append(json.loads(line))\n",
    "\n",
    "embeddings = []\n",
    "ids = []\n",
    "for response in responses:\n",
    "    embeddings.append(response[\"response\"][\"body\"][\"data\"][0]['embedding'])\n",
    "    ids.append(response[\"custom_id\"])\n",
    "\n",
    "embeddings_dict = dict(zip(ids, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e97bad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting requests and their ids:\n",
    "jsonl_files_input = glob.glob(\"../data/embed/input/batch_requests_embed*.jsonl\")\n",
    "\n",
    "ids_input = []\n",
    "texts_input = []\n",
    "for file in jsonl_files_input:\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            request = json.loads(line)\n",
    "            ids_input.append(request[\"custom_id\"])\n",
    "            texts_input.append(request[\"body\"][\"input\"])\n",
    "\n",
    "# Create a dataframe with the embeddings and the ids\n",
    "text_to_embedding = {text: embeddings_dict[id] for text, id in zip(texts_input, ids_input)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"n_added\"] = n_added\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"n_removed\"] = n_removed\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"n_changed\"] = n_changed\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"categories_added\"] = categories_added\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"categories_removed\"] = categories_removed\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"topics\"] = topics\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"locations\"] = locations\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"templates_added\"] = tem_added\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"templates_deleted\"] = tem_deleted\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"params_changed\"] = param_change\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"text_to_summary\"] = changed_df.page_name.apply(lambda d: text_to_summary.get(d, None))\n",
      "/var/folders/hy/3zvjkf9d35q17_14pc7rwjxh0000gn/T/ipykernel_80844/3960267957.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  changed_df[\"text_to_embedding\"] = changed_df.text_to_summary.apply(lambda d: text_to_embedding.get(d, None))\n"
     ]
    }
   ],
   "source": [
    "changed_df[\"n_added\"] = n_added\n",
    "changed_df[\"n_removed\"] = n_removed\n",
    "changed_df[\"n_changed\"] = n_changed\n",
    "changed_df[\"categories_added\"] = categories_added\n",
    "changed_df[\"categories_removed\"] = categories_removed\n",
    "changed_df[\"topics\"] = topics\n",
    "changed_df[\"locations\"] = locations\n",
    "changed_df[\"templates_added\"] = tem_added\n",
    "changed_df[\"templates_deleted\"] = tem_deleted\n",
    "changed_df[\"params_changed\"] = param_change\n",
    "changed_df[\"text_to_summary\"] = changed_df.page_name.apply(lambda d: text_to_summary.get(d, None))\n",
    "changed_df[\"text_to_embedding\"] = changed_df.text_to_summary.apply(lambda d: text_to_embedding.get(d, None))\n",
    "\n",
    "\n",
    "\n",
    "# Drop the column \"parsing time\", 'n_added', 'n_removed', 'n_changed'\n",
    "changed_df = changed_df.drop(columns=['n_added', 'n_removed', 'n_changed', \"parsing time\"])\n",
    "changed_df.to_csv(\"../data/rwfork_changed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33664, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['page_name', 'status', 'lines_added', 'lines_deleted', 'lines_changed',\n",
       "       'actions', 'wiki_features', 'ruwiki_features', 'parsing_time',\n",
       "       'n_added', 'n_removed', 'n_changed', 'categories_added',\n",
       "       'categories_removed', 'topics', 'locations', 'templates_added',\n",
       "       'templates_deleted', 'params_changed', 'text_to_summary',\n",
       "       'text_to_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(changed_df.shape)\n",
    "display(changed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cdad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv:\n",
    "not_found_page_names = not_found_df.page_name.to_list() + [a[\"page_name\"] for a in error_data]\n",
    "pd.DataFrame(not_found_page_names, columns=[\"page_name\"]).to_csv(\"../data/rwfork_not_found.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85614a",
   "metadata": {},
   "source": [
    "# Dataset Documentation  \n",
    "\n",
    "This dataset contains **33,664 rows** and **17 columns**. Each row represents a Wikipedia page where content differences exist between the Russian Wikipedia and the Russian Wikipedia Fork (RuWiki). Pages without any content differences were excluded. Below are the details of the columns:  \n",
    "\n",
    "1. **`page_name`**:  \n",
    "   The name of the Wikipedia page being analyzed.  \n",
    "\n",
    "2. **`status`**:  \n",
    "   Indicates the type of edit made to the page:  \n",
    "   - **3**: Changes in metadata only (not visible in the content).  \n",
    "     - The columns `lines_added`, `lines_deleted`, and `lines_changed` will be empty.  \n",
    "   - **4**: Changes in content.  \n",
    "     - At least one of the columns `lines_added`, `lines_deleted`, or `lines_changed` will contain data.  \n",
    "\n",
    "3. **`lines_added`**:  \n",
    "   A list of sentences (text pieces) that were added to RuWiki compared to Wikipedia.  \n",
    "\n",
    "4. **`lines_deleted`**:  \n",
    "   A list of sentences (text pieces) that were removed from RuWiki compared to Wikipedia.  \n",
    "\n",
    "5. **`lines_changed`**:  \n",
    "   A list of sentence pairs representing changes made in RuWiki compared to Wikipedia.  \n",
    "   - The first element of each pair is the original sentence from Wikipedia.  \n",
    "   - The second element is the modified sentence from RuWiki.  \n",
    "\n",
    "6. **`actions`**:  \n",
    "   A list of actions performed on the page to transition from Wikipedia to RuWiki.  \n",
    "   - These actions were extracted automatically using the `mwedittypes` library.  \n",
    "\n",
    "7. **`wiki_features`**:  \n",
    "   Additional metadata about the page, extracted from Wikipedia.  \n",
    "\n",
    "8. **`ruwiki_features`**:  \n",
    "   Additional metadata about the page, extracted from RuWiki.  \n",
    "\n",
    "9. **`categories_added`**:\n",
    "      The list of categories added to the page in RuWiki compared to Wikipedia.\n",
    "\n",
    "10. **`categories_removed`**:\n",
    "      The list of categories removed from the page in RuWiki compared to Wikipedia.\n",
    "\n",
    "11. **`topics`**:\n",
    "      The list of topics related to the Russian Wikipedia page (not Ruwiki page). It is extracted using https://api.wikimedia.org/wiki/Lift_Wing_API/Reference/Get_articletopic_outlink_prediction API.\n",
    "\n",
    "12. **`locations`**:\n",
    "      The list of locations related to the Russian Wikipedia page (not Ruwiki page).\n",
    "\n",
    "13. **`templates_added`**:\n",
    "      The list of templates added to the page in RuWiki compared to Wikipedia.\n",
    "\n",
    "14. **`templates_deleted`**:\n",
    "      The list of templates deleted from the page in RuWiki compared to Wikipedia.\n",
    "\n",
    "15. **`params_changed`**:\n",
    "      The list of parameters changed in the templates of the page in RuWiki compared to Wikipedia.\n",
    "\n",
    "16. **`text_to_summary`**:\n",
    "      The summary of the edit made to transition from Wikipedia to RuWiki. It is created using OpenAI API feeding all the changes made to the page.\n",
    "\n",
    "17. **`text_to_embedding`**:\n",
    "      The embedding of the summary of the edit made to transition from Wikipedia to RuWiki. It is created using OpenAI API text of the summary. Can be used for search or clustering.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
